pacman::p_load(tidyverse,
janitor,
countrycode,
DataExplorer)
# Load data
culture_data <- read_csv("data/culture.csv") %>% clean_names()
wvs_regions_data <- read_csv("data/wvs_regions.csv") %>% clean_names()
co2_data <- read_csv("data/owid_co2_data.csv") %>% clean_names()
pew_climate_data <- read_csv("data/pew_climate.csv") %>% clean_names()
population_data <- read_csv("data/population_nd_gain.csv") %>% clean_names()
vulnerability_data <- read_csv("data/vulnerability_nd_gain.csv") %>% clean_names()
gdp_capita_data <- read_csv("data/gdp_nd_gain.csv") %>% clean_names()
View(co2_data)
View(culture_data)
View(dat)
#########################
# Add ISO CODE to data
#########################
culture_data$iso_code <- countrycode(culture_data$country, origin = 'country.name', destination = 'iso3c')
wvs_regions_data$iso_code <- countrycode(wvs_regions_data$country, origin = 'country.name', destination = 'iso3c')
pew_climate_data$iso_code <- countrycode(pew_climate_data$country, origin = 'country.name', destination = 'iso3c')
names(population_data)[names(population_data) == "iso3"] <- "iso_code"
names(vulnerability_data)[names(vulnerability_data) == "iso3"] <- "iso_code"
names(gdp_capita_data)[names(gdp_capita_data) == "iso3"] <- "iso_code"
names(gdp_capita_data)[names(gdp_capita_data) == "gdp"] <- "gdp_capita"
View(wvs_regions_data)
#########################
# Wide versus long data
#########################
## Population data
head(population_data)
head(gdp_capita_data)
population_data_long <- population_data %>%
pivot_longer(cols = -c(iso_code, name), names_to = "year", values_to = "population") %>%
mutate(year = as.numeric(substr(year, 2, 5)))
gdp_capita_data_long <- gdp_capita_data %>%
pivot_longer(cols = -c(iso_code, name), names_to = "year", values_to = "gdp_capita") %>%
mutate(year = as.numeric(substr(year, 2, 5)))
population_data_wide <- population_data_long %>%
pivot_wider(names_from = year, values_from = population)
gdp_capita_data_wide <- gdp_capita_data_long %>%
pivot_wider(names_from = year, values_from = gdp_capita)
View(gdp_capita_data)
View(gdp_capita_data_long)
View(gdp_capita_data_wide)
View(gdp_capita_data_long)
View(gdp_capita_data_wide)
View(gdp_capita_data_long)
View(gdp_capita_data_long)
##################
# Combining data
##################
df_animals <- data.frame(numbers = c(1, 2, 3, 4), letters = c("a", "b", "c", "d"), animals = c("dog", "cat", "bird", "ape"))
df_colors <- data.frame(numbers = c(4, 5, 6, 7), letters = c("c", "d", "e", "f"), colors = c("black", "red", "blue", "green"))
## Row bind, column bind, bind rows, bind columns
df_animals_colors <- rbind(df_animals, df_colors)
df_animals_colors <- bind_rows(df_animals, df_colors)
df_animals_colors <- cbind(df_animals, df_colors)
df_animals_colors <- bind_cols(df_animals, df_colors)
View(df_animals_colors)
View(df_colors)
df_animals_colors <- bind_cols(df_animals, df_colors)
View(df_animals_colors)
## Join data
inner_join(df_animals, df_colors, by = "numbers")
full_join(df_animals, df_colors, by = "numbers")
left_join(df_animals, df_colors, by = "numbers")
right_join(df_animals, df_colors, by = "numbers")
semi_join(df_animals, df_colors, by = "numbers")
anti_join(df_animals, df_colors, by = "numbers")
anti_join(df_colors, df_animals, by = "numbers")
View(df_animals_colors)
View(df_animals)
View(df_animals_colors)
View(df_animals)
## Merge data
#In SQL database terminology, the
# Default: all = FALSE gives a natural join, a special case of an inner join.
# all.x = TRUE gives a left (outer) join,
# all.y = TRUE a right (outer) join,
# both (all = TRUE) a full join.
merge(df_animals, df_colors, by = "numbers", all = FALSE)
merge(df_animals, df_colors, by = "numbers", all.x = TRUE)
merge(df_animals, df_colors, by = "numbers", all.y = TRUE)
merge(df_animals, df_colors, by = "numbers", all = TRUE)
#########################
# Combining our data
#########################
## Combine culture and gdp data
culture_gdp_2020_data <- culture_data %>% select(-c(country)) %>%
left_join(subset(gdp_capita_data, select=c(iso_code, x2020)), by = c("iso_code"))
culture_gdp_2020_data <- culture_gdp_2020_data %>% select(iso_code, x2020, everything())
names(culture_gdp_2020_data)[names(culture_gdp_2020_data) == "x2020"] <- "gdp_capita_2020"
## Combine culture, gdp, and population data
culture_gdp_pop_2020_data <- culture_gdp_2020_data %>%
left_join(subset(population_data, select=c(iso_code, x2020)), by = c("iso_code")) %>%
select(iso_code, x2020, everything())
names(culture_gdp_pop_2020_data)[names(culture_gdp_pop_2020_data) == "x2020"] <- "population_2020"
## Combine culture, gdp, population, and vulnerability data
culture_gdp_pop_vuln_2020_data <- culture_gdp_pop_2020_data %>%
left_join(subset(vulnerability_data, select=c(iso_code, x2020)), by = c("iso_code")) %>%
select(iso_code, x2020, everything())
names(culture_gdp_pop_vuln_2020_data)[names(culture_gdp_pop_vuln_2020_data) == "x2020"] <- "vulnerability_2020"
## Combine culture, gdp, population, vulnerability, and co2 data
culture_gdp_pop_vuln_co2_2020_data <- culture_gdp_pop_vuln_2020_data %>%
left_join(subset(co2_data[,c(1,3,5)], year==2020), by = c("iso_code")) %>% select(-c(year)) %>%
select(iso_code, co2_per_capita, everything())
culture_gdp_pop_vuln_co2_2020_data <- na.omit(culture_gdp_pop_vuln_co2_2020_data)
## Combine culture, gdp, population, vulnerability, and wvs data
culture_gdp_pop_vuln_co2_wvs_2020_data <- culture_gdp_pop_vuln_co2_2020_data %>%
left_join(subset(wvs_regions_data, select=c(iso_code, wvs_region)), by = c("iso_code")) %>%
select(iso_code, wvs_region, everything())
# Combine culture, gdp, population, vulnerability, wvs, and pew data
all_2020_data <- culture_gdp_pop_vuln_co2_wvs_2020_data %>%
left_join(subset(pew_climate_data, year==2021), by = c("iso_code")) %>% select(-c(country, year)) %>%
select(iso_code, everything())
all_2020_data <- na.omit(all_2020_data)
introduce(all_2020_data)
create_report(all_2020_data)
setwd("~/Library/CloudStorage/OneDrive-Vanderbilt/# Data Analysis/project_data_analysis")
#install.packages("pacman")
#library(pacman)
pacman::p_load(tidyverse,
devtools,
janitor,
dplyr)
dat <- read.table("GSE203554_borchelt.final.txt")
#creating a tibble and moving column names to the header
genotyping_initial <- read.table("GSE203554_borchelt.final.txt", header = TRUE)
genotype_headernames_adjusted <-
genotyping_initial %>%
rename(
WT_1 = WT_191,
WT_2 = WT_207,
WT_3 = WT_214,
WT_4 = WT_218,
WT_5 = WT_225,
APP_PSI_CNT_1 = APPPI184,
APP_PSI_CNT_2 = APPPI255,
APP_PSI_CNT_3 = APPPI262,
APP_PSI_CNT_4 = APPPI307,
APP_PSI_CNT_5 = APPPI318,
APP_PSI_BCI838_1 = APPPI198,
APP_PSI_BCI838_2 =APPPI245,
APP_PSI_BCI838_3 =APPPI288,
APP_PSI_BCI838_4 =APPPI322,
APP_PSI_PE_1 = APPPI224,
APP_PSI_PE_2 = APPPI236,
APP_PSI_PE_3 = APPPI264,
APP_PSI_PE_4 = APPPI275,
APP_PSI_PE_5 = APPPI313,
APP_PSI_BCI838_PE_1 = APPPI182,
APP_PSI_BCI838_PE_2 = APPPI286,
APP_PSI_BCI838_PE_3 =APPPI320,
APP_PSI_BCI838_PE_4 =APPPI321,
APP_PSI_BCI838_PE_5 =APPPI376
)
View(genotype_headernames_adjusted)
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load(tidyverse)
## Load in the data
data(CO2)
View(co2_data)
View(CO2)
quebec <- filter(CO2, type == Quebec)
quebec <- filter(CO2, type == "Quebec")
quebec <- filter(CO2, type == "Quebec")
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load(tidyverse)
## Load in the data
data(CO2)
quebec <- filter(CO2, type == "Quebec")
quebec <- filter(CO2, Type == "Quebec")
mississippi <- CO2 %>%
filter(Type == "Mississippi")
View(quebec)
View(mississippi)
mississippi <- CO2 %>%
filter(Type == "Mississippi") %>%
filter(conc == 175)
View(mississippi)
mississippi <- CO2 %>%
filter(Type == "Mississippi") %>%
filter(conc == 175)
select(c(Plant, uptake))
mississippi <- CO2 %>%
filter(Type == "Mississippi") %>%
filter(conc == 175) %>%
select(c(Plant, uptake))
nonchilled <- CO2 %>%
filter(Type == "nonchilled")
View(nonchilled)
nonchilled <- CO2 %>%
filter(Treatment == "nonchilled")
View(nonchilled)
mean(nonchilled$uptake)
stats <- CO2 %>%
group_by(Type)
summarize(avg_uptake = mean(uptake))
stats <- CO2 %>%
group_by(Type) %>%
summarize(avg_uptake = mean(uptake))
View(stats)
stats1 <- CO2 %>%
group_by(Type, Treatment) %>%
summarize(avg_uptake = mean(uptake))
View(stats1)
CO2$uptake_millimoles <- CO2$uptake/1000
View(CO2)
AboveMedian <- CO2 %>%
filter(uptake > median(uptake))
View(AboveMedian)
CO2 <- CO2 %>%
rename(PlantType = Type,
TreatmentType = Treatment)
)
CO2 <- CO2 %>%
rename(PlantType = Type,
TreatmentType = Treatment)
View(CO2)
CO2 <- CO2 %>%
rename(PlantType = Type,
TreatmentType = Treatment)
View(CO2)
colnames(CO2)
?pivot
?pivot_longer
CO2_long <- pivot_longer(CO2, c(TreatmentType, PlantType))
View(CO2_long)
CO2_long <- pivot_longer(CO2, c(TreatmentType, PlantType), names_to = variable, values_to = "value")
CO2_long <- pivot_longer(CO2, c(TreatmentType, PlantType), names_to = "variable", values_to = "value")
CO2_long <- pivot_longer(CO2, c(TreatmentType, PlantType), names_to = "variable", values_to = "value")
CO2_wide <- pivot_wider(CO2_long, names_from = variable, values_from = value)
View(CO2_wide)
# Scripts
source("scripts/0_clean_combine_data.R")
setwd("~/Library/CloudStorage/OneDrive-Vanderbilt/# Data Analysis/project_data_analysis")
#creating a tibble and moving column names to the header
genotyping_initial <- read.table("GSE203554_normalised.borchelt.final.txt", header = TRUE)
dat <- read.table("GSE203554_normalised.borchelt.final.txt")
#creating a tibble and moving column names to the header
genotyping_initial <- read.table("GSE203554_normalised.borchelt.final.txt", header = TRUE)
genotype_headernames_adjusted <-
genotyping_initial %>%
rename(
WT_1 = WT_191,
WT_2 = WT_207,
WT_3 = WT_214,
WT_4 = WT_218,
WT_5 = WT_225,
APP_PSI_CNT_1 = APPPI184,
APP_PSI_CNT_2 = APPPI255,
APP_PSI_CNT_3 = APPPI262,
APP_PSI_CNT_4 = APPPI307,
APP_PSI_CNT_5 = APPPI318,
APP_PSI_BCI838_1 = APPPI198,
APP_PSI_BCI838_2 =APPPI245,
APP_PSI_BCI838_3 =APPPI288,
APP_PSI_BCI838_4 =APPPI322,
APP_PSI_PE_1 = APPPI224,
APP_PSI_PE_2 = APPPI236,
APP_PSI_PE_3 = APPPI264,
APP_PSI_PE_4 = APPPI275,
APP_PSI_PE_5 = APPPI313,
APP_PSI_BCI838_PE_1 = APPPI182,
APP_PSI_BCI838_PE_2 = APPPI286,
APP_PSI_BCI838_PE_3 =APPPI320,
APP_PSI_BCI838_PE_4 =APPPI321,
APP_PSI_BCI838_PE_5 =APPPI376
)
View(genotyping_initial)
View(genotype_headernames_adjusted)
View(genotyping_initial)
View(genotype_headernames_adjusted)
#creating a tibble and moving column names to the header
gene_raw <- read.table("GSE203554_normalised.borchelt.final.txt", header = TRUE)
gene_headernames_adjusted <-
gene_raw %>%
rename(
WT_1 = WT_191,
WT_2 = WT_207,
WT_3 = WT_214,
WT_4 = WT_218,
WT_5 = WT_225,
APP_PSI_CNT_1 = APPPI184,
APP_PSI_CNT_2 = APPPI255,
APP_PSI_CNT_3 = APPPI262,
APP_PSI_CNT_4 = APPPI307,
APP_PSI_CNT_5 = APPPI318,
APP_PSI_BCI838_1 = APPPI198,
APP_PSI_BCI838_2 =APPPI245,
APP_PSI_BCI838_3 =APPPI288,
APP_PSI_BCI838_4 =APPPI322,
APP_PSI_PE_1 = APPPI224,
APP_PSI_PE_2 = APPPI236,
APP_PSI_PE_3 = APPPI264,
APP_PSI_PE_4 = APPPI275,
APP_PSI_PE_5 = APPPI313,
APP_PSI_BCI838_PE_1 = APPPI182,
APP_PSI_BCI838_PE_2 = APPPI286,
APP_PSI_BCI838_PE_3 =APPPI320,
APP_PSI_BCI838_PE_4 =APPPI321,
APP_PSI_BCI838_PE_5 =APPPI376
)
View(gene_raw)
View(gene_headernames_adjusted)
View(genotype_headernames_adjusted)
setwd("~/Library/CloudStorage/OneDrive-Vanderbilt/# Data Analysis/cpbp8306-dataanalysis-main/S08_20241017_r_statistics_viz_4")
#####################
##### Libraries #####
#####################
pacman::p_load(tidyverse,
stats,
patchwork)
#####################
##### Load data #####
#####################
schools_data <- read.csv("https://raw.githubusercontent.com/HeardLibrary/digital-scholarship/master/data/gis/wg/Metro_Nashville_Schools.csv")
human_data <- read.csv("https://gist.githubusercontent.com/baskaufs/1a7a995c1b25d6e88b45/raw/4bb17ccc5c1e62c27627833a4f25380f27d30b35/t-test.csv")
write.csv(schools_data, "data/schools_data.csv", row.names = FALSE)
write.csv(human_data, "data/human_data.csv", row.names = FALSE)
schools_data <- read_csv("data/schools_data.csv")
human_data <- read_csv("data/human_data.csv")
#####################
###### Scripts ######
#####################
## Clean raw_data file.csv and write to data/clean_file.csv
source("scripts/0_stats_ggplot2_basics.R")
# --------------------
# Base stats
# --------------------
# Single variables
mean(na.omit(schools_data$Grade.1)) # Calculates the arithmetic mean of a numeric vector.
median(na.omit(schools_data$Grade.1)) # Calculates the median of a numeric vector.
sd(na.omit(schools_data$Grade.1)) # Calculates the standard deviation of a numeric vector.
var(na.omit(schools_data$Grade.1)) # Calculates the variance of a numeric vector.
min(na.omit(schools_data$Grade.1)) # Finds the minimum value in a numeric vector.
max(na.omit(schools_data$Grade.1)) # Finds the maximum value in a numeric vector.
sum(na.omit(schools_data$Grade.1)) # Calculates the sum of the elements in a numeric vector.
# Relationships between variables
## Combine variables into data.frame
school_demographics <- na.omit(data.frame(subset(schools_data, select = c(School.Level, Zip.Code, White, Economically.Disadvantaged, Hispanic.Latino, Limited.English.Proficiency))))
school_demographics$School.Level <- as.factor(as.character(school_demographics$School.Level))
school_demographics$Zip.Code <- as.factor(as.character(school_demographics$Zip.Code))
## Correlation
cor(school_demographics$Economically.Disadvantaged, school_demographics$Limited.English.Proficiency, method = "spearman") # Calculates the correlation between two numeric vectors.
## t-test
t.test(school_demographics$Economically.Disadvantaged) # Performs a t-test on a numeric vector or two numeric vectors.
t.test(Economically.Disadvantaged ~ School.Level, data=subset(school_demographics, School.Level %in% c("High School", "Elementary School"))) # Performs a two sided t-test
## Chi-square test
chisq.test(school_demographics$Economically.Disadvantaged, school_demographics$School.Level) # Performs a chi-square test of independence on a contingency table.
school_demographics %>% group_by(School.Level) %>% summarise(count = n()) # Summarizes the number of observations in each group.
## Linear regression
model1 <- lm(Economically.Disadvantaged ~ Hispanic.Latino + Limited.English.Proficiency, data = school_demographics) # Performs linear regression on a dataset.
model1
summary(model1) # Summarizes the output of a linear regression model.
# ANOVA
anova(model1) # Performs analysis of variance (ANOVA) on a statistical model.
# --------------------
# stats package aov()
# --------------------
model2 <- lm(White ~ School.Level, data = school_demographics) # Performs linear regression on a dataset.
aov(model2) # Performs analysis of variance (ANOVA) on a statistical model.
TukeyHSD(aov(model2)) # Performs Tukey's Honest Significant Differences (HSD) test on a dataset.
# Generic R hist() function for histograms
hist(schools_data$Female)
# Assigning one of the functions to a variable
base_plot <- ggplot(data = schools_data)
base_plot + geom_histogram(mapping = aes(x = Female), binwidth = 100)
# Create a histogram using ggplot
ggplot(data = schools_data) + geom_histogram(mapping = aes(x = Female), binwidth = 100)
# Assigning one of the functions to a variable
base_plot <- ggplot(data = schools_data)
base_plot + geom_histogram(mapping = aes(x = Female), binwidth = 100)
# For multiline, must have a trailing + sign.
ggplot(data = schools_data) +
geom_histogram(mapping = aes(x = Female), binwidth = 100)
str(human_data)
plot(human_data$height ~ as.factor(human_data$grouping))
# ggplot typically uses tibbles, but doesn't seem to have a problem with a generic data frame
ggplot(data = human_data) +
geom_boxplot(mapping = aes(x = grouping, y = height))
# The standard R scatterplot uses the same syntax as for box and whisker:
# plot(y ~ x)
# with x as the limited English proficiency variable
# and y as the economically disadvantaged variable
# Both x and y must be continuous numeric values.
plot(schools_data$Economically.Disadvantaged ~ schools_data$Limited.English.Proficiency)
plot(schools_data$Economically.Disadvantaged ~ schools_data$Limited.English.Proficiency,
col = "blue", main = "Economically Disadvantaged vs Limited English Proficiency",
xlab = "Limited English Proficiency", ylab = "Economically Disadvantaged")
# Create a similar scatterplot using ggplot
ggplot(data = schools_data) +
geom_point(mapping = aes(x = Limited.English.Proficiency, y = Economically.Disadvantaged))
# Fit these two data to a linear model using
model <- lm(y ~ x)
model <- lm(schools_data$Economically.Disadvantaged ~ schools_data$Limited.English.Proficiency)
plot(schools_data$Economically.Disadvantaged ~ schools_data$Limited.English.Proficiency)
abline(model)
# Create a scatterplot using ggplot that includes the best fit line through the data
ggplot(data = schools_data, aes(x = Limited.English.Proficiency, y = Economically.Disadvantaged)) +
geom_point() +
geom_smooth(method = "lm")
# Annotate the plot with the R2 value and p-value
ggplot(data = schools_data, aes(x = Limited.English.Proficiency, y = Economically.Disadvantaged)) +
geom_point() +
geom_smooth(method = "lm") +
annotate("text", x = 600, y = 100, label = paste("R2 = ", round(summary(model)$r.squared, 2), "\n",
"p = ", round(summary(model)$coefficients[8], 4)))
# -------------------------
# Loading in data to ggplot
# -------------------------
# Create a scatterplot using ggplot that includes the best fit line through the data
# universal mapping to reduce redundancy
ggplot(data = schools_data, aes(x = Limited.English.Proficiency, y = Economically.Disadvantaged)) +
geom_point() +
geom_smooth(method = "lm")
# local mapping to specify different data and aesthetics
ggplot() +
geom_point(data = schools_data, mapping = aes(x = Limited.English.Proficiency, y = Economically.Disadvantaged)) +
geom_smooth(data = schools_data, mapping = aes(x = Limited.English.Proficiency, y = Economically.Disadvantaged), method = "lm")
# piping in data using dplyr
schools_data %>%
ggplot(aes(x = Limited.English.Proficiency, y = Economically.Disadvantaged)) +
geom_point() +
geom_smooth(method = "lm")
english_distadvantage_plot <- ggplot(data = subset(schools_data, School.Level %in% c("Charter", "Elementary School", "High School", "Middle School")),
aes(x = Limited.English.Proficiency, y = Economically.Disadvantaged)) +
geom_point() +
geom_smooth(method = "lm")
english_distadvantage_level_plot <- ggplot(data = subset(schools_data, School.Level %in% c("Charter", "Elementary School", "High School", "Middle School")),
aes(x = Limited.English.Proficiency, y = Economically.Disadvantaged, color = School.Level)) +
geom_point() +
geom_smooth(method = "lm") +
theme(legend.position = "bottom")
# patchwork package to combine plots
combo_plot <- english_distadvantage_plot + english_distadvantage_level_plot
combo_plot
combo_plot <- english_distadvantage_plot | english_distadvantage_level_plot
combo_plot
combo_plot <- english_distadvantage_plot / english_distadvantage_level_plot
combo_plot
# Annotate plot
combo_plot <- english_distadvantage_plot / english_distadvantage_level_plot
combo_plot <- combo_plot + plot_annotation(tag_levels = "a")
combo_plot
# Export plot
ggsave("output/combo_plot.png", plot = combo_plot, width = 6, height = 10, units = "in", dpi = 300)
view(schools_Data)
view(schools_data)
model3 <- lm(Economically.Disadvantaged ~ Limited.English.Proficiency, data = schools_data)
mode3
summary(model3)
model3 <- lm(Economically.Disadvantaged ~ Limited.English.Proficiency, data = schools_data)
model3
summary(model3)
aov(model3) # Performs analysis of variance (ANOVA) on a statistical model.
TukeyHSD(aov(model3)) # Performs Tukey's Honest Significant Differences (HSD) test on a dataset.
aov(model3) # Performs analysis of variance (ANOVA) on a statistical model.
TukeyHSD(aov(model3)) # Performs Tukey's Honest Significant Differences (HSD) test on a dataset.
model3 <- lm(Economically.Disadvantaged ~ Zip.Code, data = schools_data)
model3
summary(model3)
aov(model3) # Performs analysis of variance (ANOVA) on a statistical model.
TukeyHSD(aov(model3)) # Performs Tukey's Honest Significant Differences (HSD) test on a dataset.
model3 <- lm(Economically.Disadvantaged ~ School.Level, data = schools_data)
model3
summary(model3)
aov(model3) # Performs analysis of variance (ANOVA) on a statistical model.
TukeyHSD(aov(model3)) # Performs Tukey's Honest Significant Differences (HSD) test on a dataset.
ggplot(data = schools_data, aes(x = School.Level, y = Economically.Disadvantaged)) +
geom_point() +
geom_smooth(method = "lm")
#install.packages("pacman")
#library(pacman)
pacman::p_load(tidyverse,
devtools,
janitor,
dplyr)
dat <- read.table("GSE203554_normalised.borchelt.final.txt")
#creating a tibble and moving column names to the header
gene_raw <- read.table("GSE203554_normalised.borchelt.final.txt", header = TRUE)
gene_headernames_adjusted <-
gene_raw %>%
rename(
WT_1 = WT_191,
WT_2 = WT_207,
WT_3 = WT_214,
WT_4 = WT_218,
WT_5 = WT_225,
APP_PSI_CNT_1 = APPPI184,
APP_PSI_CNT_2 = APPPI255,
APP_PSI_CNT_3 = APPPI262,
APP_PSI_CNT_4 = APPPI307,
APP_PSI_CNT_5 = APPPI318,
APP_PSI_BCI838_1 = APPPI198,
APP_PSI_BCI838_2 =APPPI245,
APP_PSI_BCI838_3 =APPPI288,
APP_PSI_BCI838_4 =APPPI322,
APP_PSI_PE_1 = APPPI224,
APP_PSI_PE_2 = APPPI236,
APP_PSI_PE_3 = APPPI264,
APP_PSI_PE_4 = APPPI275,
APP_PSI_PE_5 = APPPI313,
APP_PSI_BCI838_PE_1 = APPPI182,
APP_PSI_BCI838_PE_2 = APPPI286,
APP_PSI_BCI838_PE_3 =APPPI320,
APP_PSI_BCI838_PE_4 =APPPI321,
APP_PSI_BCI838_PE_5 =APPPI376
)
View(gene_headernames_adjusted)
